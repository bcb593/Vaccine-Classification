{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256c0ffa",
   "metadata": {},
   "source": [
    "# Classification of Pro-vax vs Anti-vax vs Neutral Tweets using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3177c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "datadir = '../data/'\n",
    "resultdir = '../results/'\n",
    "numFolds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d56973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795772f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3038, 4)\n",
      "Loading augmented data\n",
      "Pro-vs-anti class distribution:\n",
      " 0    1572\n",
      " 1     982\n",
      "-1     484\n",
      "Name: class, dtype: int64\n",
      " 0    0.517446\n",
      " 1    0.323239\n",
      "-1    0.159315\n",
      "Name: class, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>list</th>\n",
       "      <th>rowid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Iâ€™m no doctor â€” I just wonâ€™t be putting a â€œflu...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@redditships My dad has cancer and is immunoco...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I see the Flu Shot peddlers are out in full fo...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Iâ€™ve spent 3 hours of my life trying to get a ...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Just got the flu shot. Do you want to hear the...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3033</th>\n",
       "      <td>0</td>\n",
       "      <td>@SianGriffiths6 @TheClash709 @thesundaytimes @...</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>3033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>0</td>\n",
       "      <td>How much would you pay to stay alive? How much...</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>3034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>0</td>\n",
       "      <td>Imagine if every prescription drug ever produc...</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>3035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>0</td>\n",
       "      <td>@JoeKoffee Cops (esp corrupt ones!), insurance...</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>3036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>0</td>\n",
       "      <td>being \"trans\" will lead to autismðŸŒŸ</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>3037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3038 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                              tweet       list  \\\n",
       "0         1  Iâ€™m no doctor â€” I just wonâ€™t be putting a â€œflu...  [1, 0, 0]   \n",
       "1         1  @redditships My dad has cancer and is immunoco...  [1, 0, 0]   \n",
       "2         1  I see the Flu Shot peddlers are out in full fo...  [1, 0, 0]   \n",
       "3         1  Iâ€™ve spent 3 hours of my life trying to get a ...  [1, 0, 0]   \n",
       "4         1  Just got the flu shot. Do you want to hear the...  [1, 0, 0]   \n",
       "...     ...                                                ...        ...   \n",
       "3033      0  @SianGriffiths6 @TheClash709 @thesundaytimes @...  [0, 0, 1]   \n",
       "3034      0  How much would you pay to stay alive? How much...  [0, 0, 1]   \n",
       "3035      0  Imagine if every prescription drug ever produc...  [0, 0, 1]   \n",
       "3036      0  @JoeKoffee Cops (esp corrupt ones!), insurance...  [0, 0, 1]   \n",
       "3037      0                 being \"trans\" will lead to autismðŸŒŸ  [0, 0, 1]   \n",
       "\n",
       "      rowid  \n",
       "0         0  \n",
       "1         1  \n",
       "2         2  \n",
       "3         3  \n",
       "4         4  \n",
       "...     ...  \n",
       "3033   3033  \n",
       "3034   3034  \n",
       "3035   3035  \n",
       "3036   3036  \n",
       "3037   3037  \n",
       "\n",
       "[3038 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classList(classval):\n",
    "    if classval == 1:\n",
    "        return [1,0,0]\n",
    "    elif classval == -1:\n",
    "        return [0,1,0]\n",
    "    else:\n",
    "        return [0,0,1]\n",
    "\n",
    "augdata = pd.read_csv(datadir + \"pro_anti_augmented.csv\",encoding = 'utf-8', header = 'infer')\n",
    "print(augdata.shape)\n",
    "augdata['tweet']= augdata['X']\n",
    "augdata = augdata.drop(columns=['screen_name','TweetID','X'])\n",
    "\n",
    "print('Loading augmented data')\n",
    "print('Pro-vs-anti class distribution:')\n",
    "distrib = augdata['class'].value_counts()\n",
    "print(distrib)\n",
    "probs = distrib/sum(distrib)\n",
    "print(probs)\n",
    "\n",
    "temp = augdata['class'].apply(classList)\n",
    "augdata['list'] = temp\n",
    "augdata['rowid'] = augdata.index\n",
    "augdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a8469e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>list</th>\n",
       "      <th>rowid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@DickDugan @maddow They make much more selling...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@theheraldsun How does an unvaccinated person ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@cameronjowens @leighsales We have much higher...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Officials I trust fear there is not enough int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My daughter 30 years old, care worker. Double ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4837</th>\n",
       "      <td>M?ori tribe tells anti-Covid vaccine protester...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>4837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>Anti-vaccine protesters display Nazi symbols o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>4838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>@Joc_face @TravisR96776163 @ElijahSchaffer If ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>4839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4840</th>\n",
       "      <td>@ponderousthings @1NewsNZ @jordyn_rudd \"The SA...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>4840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>@JohnAndersonAO My concern is that it's a COVI...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>4841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4842 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  class       list  \\\n",
       "0     @DickDugan @maddow They make much more selling...     -1  [0, 1, 0]   \n",
       "1     @theheraldsun How does an unvaccinated person ...     -1  [0, 1, 0]   \n",
       "2     @cameronjowens @leighsales We have much higher...      1  [1, 0, 0]   \n",
       "3     Officials I trust fear there is not enough int...      1  [1, 0, 0]   \n",
       "4     My daughter 30 years old, care worker. Double ...     -1  [0, 1, 0]   \n",
       "...                                                 ...    ...        ...   \n",
       "4837  M?ori tribe tells anti-Covid vaccine protester...      0  [0, 0, 1]   \n",
       "4838  Anti-vaccine protesters display Nazi symbols o...      0  [0, 0, 1]   \n",
       "4839  @Joc_face @TravisR96776163 @ElijahSchaffer If ...      1  [1, 0, 0]   \n",
       "4840  @ponderousthings @1NewsNZ @jordyn_rudd \"The SA...      0  [0, 0, 1]   \n",
       "4841  @JohnAndersonAO My concern is that it's a COVI...     -1  [0, 1, 0]   \n",
       "\n",
       "      rowid  \n",
       "0         0  \n",
       "1         1  \n",
       "2         2  \n",
       "3         3  \n",
       "4         4  \n",
       "...     ...  \n",
       "4837   4837  \n",
       "4838   4838  \n",
       "4839   4839  \n",
       "4840   4840  \n",
       "4841   4841  \n",
       "\n",
       "[4842 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(datadir + \"vax.csv\")\n",
    "df = df.drop(columns=['vax_class','vaccine'])\n",
    "df['list'] = df[df.columns[3:]].values.tolist()\n",
    "new_df = df[['tweet', 'class', 'list']].copy()\n",
    "new_df['rowid'] = new_df.index\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7cfaf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 150\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a7d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.tweet\n",
    "        self.targets = self.data.list\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'rowid': self.data.loc[index, 'rowid'],\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb2c128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 3)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db82c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fd3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch, data_loader):\n",
    "    model.train()\n",
    "    for _,data in enumerate(data_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "415bffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(data, augdata, aug_prop = [0, 0, 0], aug_type=1):\n",
    "    classes = [-1,0,1]\n",
    "    if aug_type > 0:\n",
    "        data = pd.concat([data, augdata], ignore_index=True)\n",
    "        y_train = pd.concat((y_train, y_aug))\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        I = np.where(data['class']==classes[i])[0]\n",
    "        for j in range(aug_prop[i]):\n",
    "            i1 = np.random.randint(I.shape[0])\n",
    "            i2 = np.random.randint(I.shape[0])\n",
    "            txt = data.loc[I[i1],'tweet'] + ' ' + data.loc[I[i2],'tweet']\n",
    "            temp = pd.Series(txt, name='tweet').to_frame()\n",
    "            temp['class'] = classes[i]\n",
    "            data = pd.concat([data, temp], ignore_index=True)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "998f46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, data_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    dlist = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            dlist.extend(data['rowid'].cpu().detach().numpy().tolist())\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return dlist, fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04e07618",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Training class distribution:\n",
      "-1    1764\n",
      " 1    1694\n",
      " 0    1599\n",
      "Name: class, dtype: int64\n",
      "-1    0.348823\n",
      " 1    0.334981\n",
      " 0    0.316195\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      " 1    181\n",
      " 0    166\n",
      "-1    138\n",
      "Name: class, dtype: int64\n",
      " 1    0.373196\n",
      " 0    0.342268\n",
      "-1    0.284536\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (4842, 4)\n",
      "TRAIN Dataset: (5057, 4)\n",
      "TEST Dataset: (485, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7574185729026794\n",
      "Epoch: 1, Loss:  0.5307643413543701\n",
      "Epoch: 2, Loss:  0.37962228059768677\n",
      "Epoch: 3, Loss:  0.09457385540008545\n",
      "Epoch: 4, Loss:  0.023066159337759018\n",
      "Epoch: 5, Loss:  0.02703840099275112\n",
      "Epoch: 6, Loss:  0.0077627371065318584\n",
      "Epoch: 7, Loss:  0.05159822106361389\n",
      "Epoch: 8, Loss:  0.017645718529820442\n",
      "Epoch: 9, Loss:  0.0029216192197054625\n",
      "Epoch: 10, Loss:  0.0055831847712397575\n",
      "Epoch: 11, Loss:  0.020425375550985336\n",
      "Epoch: 12, Loss:  0.0016577604692429304\n",
      "Epoch: 13, Loss:  0.002340245060622692\n",
      "Epoch: 14, Loss:  0.0013249560724943876\n",
      "Accuracy Score = 0.7917525773195876\n",
      "F1 Score (Micro) = 0.7983367983367983\n",
      "F1 Score (Macro) = 0.791097014179665\n",
      "Confusion matrix:\n",
      "[[[257  47]\n",
      "  [ 24 157]]\n",
      "\n",
      " [[335  12]\n",
      "  [ 51  87]]\n",
      "\n",
      " [[285  34]\n",
      "  [ 26 140]]]\n",
      "\n",
      "Fold 2\n",
      "Training class distribution:\n",
      "-1    1754\n",
      " 1    1693\n",
      " 0    1610\n",
      "Name: class, dtype: int64\n",
      "-1    0.346846\n",
      " 1    0.334783\n",
      " 0    0.318371\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      " 1    182\n",
      " 0    155\n",
      "-1    148\n",
      "Name: class, dtype: int64\n",
      " 1    0.375258\n",
      " 0    0.319588\n",
      "-1    0.305155\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (4842, 4)\n",
      "TRAIN Dataset: (5057, 4)\n",
      "TEST Dataset: (485, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6897427439689636\n",
      "Epoch: 1, Loss:  0.451312392950058\n",
      "Epoch: 2, Loss:  0.2195434868335724\n",
      "Epoch: 3, Loss:  0.2710755467414856\n",
      "Epoch: 4, Loss:  0.05866151303052902\n",
      "Epoch: 5, Loss:  0.01553037017583847\n",
      "Epoch: 6, Loss:  0.011355718597769737\n",
      "Epoch: 7, Loss:  0.005597327835857868\n",
      "Epoch: 8, Loss:  0.014491323381662369\n",
      "Epoch: 9, Loss:  0.003972050733864307\n",
      "Epoch: 10, Loss:  0.283845454454422\n",
      "Epoch: 11, Loss:  0.005987305194139481\n",
      "Epoch: 12, Loss:  0.0027289590798318386\n",
      "Epoch: 13, Loss:  0.001980640459805727\n",
      "Epoch: 14, Loss:  0.0016065735835582018\n",
      "Accuracy Score = 0.7896907216494845\n",
      "F1 Score (Micro) = 0.7966804979253113\n",
      "F1 Score (Macro) = 0.7928362147112148\n",
      "Confusion matrix:\n",
      "[[[247  56]\n",
      "  [ 24 158]]\n",
      "\n",
      " [[323  14]\n",
      "  [ 54  94]]\n",
      "\n",
      " [[305  25]\n",
      "  [ 23 132]]]\n",
      "\n",
      "Fold 3\n",
      "Training class distribution:\n",
      "-1    1765\n",
      " 1    1664\n",
      " 0    1629\n",
      "Name: class, dtype: int64\n",
      "-1    0.348952\n",
      " 1    0.328984\n",
      " 0    0.322064\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      " 1    211\n",
      "-1    137\n",
      " 0    136\n",
      "Name: class, dtype: int64\n",
      " 1    0.435950\n",
      "-1    0.283058\n",
      " 0    0.280992\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (4842, 4)\n",
      "TRAIN Dataset: (5058, 4)\n",
      "TEST Dataset: (484, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7205302119255066\n",
      "Epoch: 1, Loss:  0.5164095759391785\n",
      "Epoch: 2, Loss:  0.24578754603862762\n",
      "Epoch: 3, Loss:  0.046165697276592255\n",
      "Epoch: 4, Loss:  0.03772345185279846\n",
      "Epoch: 5, Loss:  0.01348819024860859\n",
      "Epoch: 6, Loss:  0.007164060138165951\n",
      "Epoch: 7, Loss:  0.3766392171382904\n",
      "Epoch: 8, Loss:  0.06327370554208755\n",
      "Epoch: 9, Loss:  0.003065291792154312\n",
      "Epoch: 10, Loss:  0.05517382547259331\n",
      "Epoch: 11, Loss:  0.003823998384177685\n",
      "Epoch: 12, Loss:  0.002751957392320037\n",
      "Epoch: 13, Loss:  0.003306345781311393\n",
      "Epoch: 14, Loss:  0.0023069935850799084\n",
      "Accuracy Score = 0.8388429752066116\n",
      "F1 Score (Micro) = 0.8518134715025907\n",
      "F1 Score (Macro) = 0.8478281842767824\n",
      "Confusion matrix:\n",
      "[[[242  31]\n",
      "  [ 25 186]]\n",
      "\n",
      " [[324  23]\n",
      "  [ 33 104]]\n",
      "\n",
      " [[332  16]\n",
      "  [ 15 121]]]\n",
      "\n",
      "Fold 4\n",
      "Training class distribution:\n",
      "-1    1762\n",
      " 1    1703\n",
      " 0    1593\n",
      "Name: class, dtype: int64\n",
      "-1    0.348359\n",
      " 1    0.336694\n",
      " 0    0.314947\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      " 1    172\n",
      " 0    172\n",
      "-1    140\n",
      "Name: class, dtype: int64\n",
      " 1    0.355372\n",
      " 0    0.355372\n",
      "-1    0.289256\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (4842, 4)\n",
      "TRAIN Dataset: (5058, 4)\n",
      "TEST Dataset: (484, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7283346056938171\n",
      "Epoch: 1, Loss:  0.6413623690605164\n",
      "Epoch: 2, Loss:  0.19911178946495056\n",
      "Epoch: 3, Loss:  0.1009327620267868\n",
      "Epoch: 4, Loss:  0.11905509233474731\n",
      "Epoch: 5, Loss:  0.014507199637591839\n",
      "Epoch: 6, Loss:  0.01083921454846859\n",
      "Epoch: 7, Loss:  0.00963563285768032\n",
      "Epoch: 8, Loss:  0.005300753749907017\n",
      "Epoch: 9, Loss:  0.0028306134045124054\n",
      "Epoch: 10, Loss:  0.004030347801744938\n",
      "Epoch: 11, Loss:  0.001765971421264112\n",
      "Epoch: 12, Loss:  0.0016210083849728107\n",
      "Epoch: 13, Loss:  0.002230075653642416\n",
      "Epoch: 14, Loss:  0.0013251041527837515\n",
      "Accuracy Score = 0.8223140495867769\n",
      "F1 Score (Micro) = 0.8319502074688797\n",
      "F1 Score (Macro) = 0.8315366050274048\n",
      "Confusion matrix:\n",
      "[[[279  33]\n",
      "  [ 29 143]]\n",
      "\n",
      " [[323  21]\n",
      "  [ 27 113]]\n",
      "\n",
      " [[287  25]\n",
      "  [ 27 145]]]\n",
      "\n",
      "Fold 5\n",
      "Training class distribution:\n",
      "-1    1760\n",
      " 1    1691\n",
      " 0    1607\n",
      "Name: class, dtype: int64\n",
      "-1    0.347964\n",
      " 1    0.334322\n",
      " 0    0.317715\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      " 1    184\n",
      " 0    158\n",
      "-1    142\n",
      "Name: class, dtype: int64\n",
      " 1    0.380165\n",
      " 0    0.326446\n",
      "-1    0.293388\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (4842, 4)\n",
      "TRAIN Dataset: (5058, 4)\n",
      "TEST Dataset: (484, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7713998556137085\n",
      "Epoch: 1, Loss:  0.3557954728603363\n",
      "Epoch: 2, Loss:  0.17199425399303436\n",
      "Epoch: 3, Loss:  0.1783718764781952\n",
      "Epoch: 4, Loss:  0.01905936934053898\n",
      "Epoch: 5, Loss:  0.009451339952647686\n",
      "Epoch: 6, Loss:  0.01100762840360403\n",
      "Epoch: 7, Loss:  0.008906076662242413\n",
      "Epoch: 8, Loss:  0.0033414780627936125\n",
      "Epoch: 9, Loss:  0.004345997236669064\n",
      "Epoch: 10, Loss:  0.018151642754673958\n",
      "Epoch: 11, Loss:  0.004111660644412041\n",
      "Epoch: 12, Loss:  0.0020864687394350767\n",
      "Epoch: 13, Loss:  0.09593334794044495\n",
      "Epoch: 14, Loss:  0.0017705465434119105\n",
      "Accuracy Score = 0.7954545454545454\n",
      "F1 Score (Micro) = 0.8020833333333333\n",
      "F1 Score (Macro) = 0.8006679245617297\n",
      "Confusion matrix:\n",
      "[[[279  21]\n",
      "  [ 56 128]]\n",
      "\n",
      " [[308  34]\n",
      "  [ 30 112]]\n",
      "\n",
      " [[290  36]\n",
      "  [ 13 145]]]\n",
      "\n",
      "Fold 6\n",
      "Training class distribution:\n",
      "-1    1772\n",
      " 1    1679\n",
      " 0    1607\n",
      "Name: class, dtype: int64\n",
      "-1    0.350336\n",
      " 1    0.331949\n",
      " 0    0.317715\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      " 1    196\n",
      " 0    158\n",
      "-1    130\n",
      "Name: class, dtype: int64\n",
      " 1    0.404959\n",
      " 0    0.326446\n",
      "-1    0.268595\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (4842, 4)\n",
      "TRAIN Dataset: (5058, 4)\n",
      "TEST Dataset: (484, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6273123025894165\n",
      "Epoch: 1, Loss:  0.5169473886489868\n",
      "Epoch: 2, Loss:  0.42934808135032654\n",
      "Epoch: 3, Loss:  0.2000660002231598\n",
      "Epoch: 4, Loss:  0.031231652945280075\n",
      "Epoch: 5, Loss:  0.019908733665943146\n",
      "Epoch: 6, Loss:  0.014909572899341583\n",
      "Epoch: 7, Loss:  0.006176270544528961\n",
      "Epoch: 8, Loss:  0.0036442596465349197\n",
      "Epoch: 9, Loss:  0.005515416618436575\n",
      "Epoch: 10, Loss:  0.0050139399245381355\n",
      "Epoch: 11, Loss:  0.002757232403382659\n",
      "Epoch: 12, Loss:  0.0031923342030495405\n",
      "Epoch: 13, Loss:  0.001811258029192686\n",
      "Epoch: 14, Loss:  0.0013450270052999258\n",
      "Accuracy Score = 0.7975206611570248\n",
      "F1 Score (Micro) = 0.8016614745586709\n",
      "F1 Score (Macro) = 0.7899651788540677\n",
      "Confusion matrix:\n",
      "[[[249  39]\n",
      "  [ 26 170]]\n",
      "\n",
      " [[333  21]\n",
      "  [ 47  83]]\n",
      "\n",
      " [[293  33]\n",
      "  [ 25 133]]]\n",
      "\n",
      "Fold 7\n",
      "Training class distribution:\n",
      "-1    1756\n",
      " 1    1686\n",
      " 0    1616\n",
      "Name: class, dtype: int64\n",
      "-1    0.347173\n",
      " 1    0.333333\n",
      " 0    0.319494\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      " 1    189\n",
      " 0    149\n",
      "-1    146\n",
      "Name: class, dtype: int64\n",
      " 1    0.390496\n",
      " 0    0.307851\n",
      "-1    0.301653\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (4842, 4)\n",
      "TRAIN Dataset: (5058, 4)\n",
      "TEST Dataset: (484, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6387871503829956\n",
      "Epoch: 1, Loss:  0.46367138624191284\n",
      "Epoch: 2, Loss:  0.1128455251455307\n",
      "Epoch: 3, Loss:  0.08668786287307739\n",
      "Epoch: 4, Loss:  0.017573688179254532\n",
      "Epoch: 5, Loss:  0.3624812960624695\n",
      "Epoch: 6, Loss:  0.01180395856499672\n",
      "Epoch: 7, Loss:  0.005551550537347794\n",
      "Epoch: 8, Loss:  0.0044354163110256195\n",
      "Epoch: 9, Loss:  0.0026176718529313803\n",
      "Epoch: 10, Loss:  0.0026172189973294735\n",
      "Epoch: 11, Loss:  0.0053107901476323605\n",
      "Epoch: 12, Loss:  0.0023703603073954582\n",
      "Epoch: 13, Loss:  0.004318095277994871\n",
      "Epoch: 14, Loss:  0.00170547841116786\n",
      "Accuracy Score = 0.8119834710743802\n",
      "F1 Score (Micro) = 0.8220603537981269\n",
      "F1 Score (Macro) = 0.8204144848750429\n",
      "Confusion matrix:\n",
      "[[[257  38]\n",
      "  [ 27 162]]\n",
      "\n",
      " [[324  14]\n",
      "  [ 38 108]]\n",
      "\n",
      " [[305  30]\n",
      "  [ 24 125]]]\n",
      "\n",
      "Fold 8\n",
      "Training class distribution:\n",
      "-1    1750\n",
      " 1    1696\n",
      " 0    1612\n",
      "Name: class, dtype: int64\n",
      "-1    0.345987\n",
      " 1    0.335310\n",
      " 0    0.318703\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      " 1    179\n",
      " 0    153\n",
      "-1    152\n",
      "Name: class, dtype: int64\n",
      " 1    0.369835\n",
      " 0    0.316116\n",
      "-1    0.314050\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (4842, 4)\n",
      "TRAIN Dataset: (5058, 4)\n",
      "TEST Dataset: (484, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6708971261978149\n",
      "Epoch: 1, Loss:  0.4802745580673218\n",
      "Epoch: 2, Loss:  0.09759895503520966\n",
      "Epoch: 3, Loss:  0.07327219843864441\n",
      "Epoch: 4, Loss:  0.03144267946481705\n",
      "Epoch: 5, Loss:  0.012761122547090054\n",
      "Epoch: 6, Loss:  0.01070667989552021\n",
      "Epoch: 7, Loss:  0.008353101089596748\n",
      "Epoch: 8, Loss:  0.003592776833102107\n",
      "Epoch: 9, Loss:  0.003362707793712616\n",
      "Epoch: 10, Loss:  0.002441544784232974\n",
      "Epoch: 11, Loss:  0.0017692440887913108\n",
      "Epoch: 12, Loss:  0.001761598396115005\n",
      "Epoch: 13, Loss:  0.0013148672878742218\n",
      "Epoch: 14, Loss:  0.0012089149095118046\n",
      "Accuracy Score = 0.8202479338842975\n",
      "F1 Score (Micro) = 0.8257261410788383\n",
      "F1 Score (Macro) = 0.8250533428165007\n",
      "Confusion matrix:\n",
      "[[[262  43]\n",
      "  [ 21 158]]\n",
      "\n",
      " [[313  19]\n",
      "  [ 35 117]]\n",
      "\n",
      " [[311  20]\n",
      "  [ 30 123]]]\n",
      "\n",
      "Fold 9\n",
      "Training class distribution:\n",
      "-1    1771\n",
      " 1    1683\n",
      " 0    1604\n",
      "Name: class, dtype: int64\n",
      "-1    0.350138\n",
      " 1    0.332740\n",
      " 0    0.317121\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      " 1    192\n",
      " 0    161\n",
      "-1    131\n",
      "Name: class, dtype: int64\n",
      " 1    0.396694\n",
      " 0    0.332645\n",
      "-1    0.270661\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (4842, 4)\n",
      "TRAIN Dataset: (5058, 4)\n",
      "TEST Dataset: (484, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6631806492805481\n",
      "Epoch: 1, Loss:  0.49677976965904236\n",
      "Epoch: 2, Loss:  0.3931460380554199\n",
      "Epoch: 3, Loss:  0.164155974984169\n",
      "Epoch: 4, Loss:  0.1792258620262146\n",
      "Epoch: 5, Loss:  0.008553668856620789\n",
      "Epoch: 6, Loss:  0.007165650837123394\n",
      "Epoch: 7, Loss:  0.010205637663602829\n",
      "Epoch: 8, Loss:  0.008232878521084785\n",
      "Epoch: 9, Loss:  0.00435230415314436\n",
      "Epoch: 10, Loss:  0.0033588637597858906\n",
      "Epoch: 11, Loss:  0.002432773821055889\n",
      "Epoch: 12, Loss:  0.0016807178035378456\n",
      "Epoch: 13, Loss:  0.0015685041435062885\n",
      "Epoch: 14, Loss:  0.002277905587106943\n",
      "Accuracy Score = 0.8491735537190083\n",
      "F1 Score (Micro) = 0.8609958506224067\n",
      "F1 Score (Macro) = 0.8592878195531909\n",
      "Confusion matrix:\n",
      "[[[277  15]\n",
      "  [ 32 160]]\n",
      "\n",
      " [[324  29]\n",
      "  [ 16 115]]\n",
      "\n",
      " [[302  21]\n",
      "  [ 21 140]]]\n",
      "\n",
      "Fold 10\n",
      "Training class distribution:\n",
      "-1    1764\n",
      " 1    1686\n",
      " 0    1608\n",
      "Name: class, dtype: int64\n",
      "-1    0.348754\n",
      " 1    0.333333\n",
      " 0    0.317912\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      " 1    189\n",
      " 0    157\n",
      "-1    138\n",
      "Name: class, dtype: int64\n",
      " 1    0.390496\n",
      " 0    0.324380\n",
      "-1    0.285124\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (4842, 4)\n",
      "TRAIN Dataset: (5058, 4)\n",
      "TEST Dataset: (484, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.8256053924560547\n",
      "Epoch: 1, Loss:  0.40501832962036133\n",
      "Epoch: 2, Loss:  0.1368398815393448\n",
      "Epoch: 3, Loss:  0.11221623420715332\n",
      "Epoch: 4, Loss:  0.2938615381717682\n",
      "Epoch: 5, Loss:  0.04750397428870201\n",
      "Epoch: 6, Loss:  0.19520670175552368\n",
      "Epoch: 7, Loss:  0.21889083087444305\n",
      "Epoch: 8, Loss:  0.005484272725880146\n",
      "Epoch: 9, Loss:  0.007018472068011761\n",
      "Epoch: 10, Loss:  0.00233721686527133\n",
      "Epoch: 11, Loss:  0.0019977649208158255\n",
      "Epoch: 12, Loss:  0.0014762053033336997\n",
      "Epoch: 13, Loss:  0.06719474494457245\n",
      "Epoch: 14, Loss:  0.0017934240167960525\n",
      "Accuracy Score = 0.8533057851239669\n",
      "F1 Score (Micro) = 0.8621761658031089\n",
      "F1 Score (Macro) = 0.8574866310160427\n",
      "Confusion matrix:\n",
      "[[[271  24]\n",
      "  [ 17 172]]\n",
      "\n",
      " [[322  24]\n",
      "  [ 28 110]]\n",
      "\n",
      " [[310  17]\n",
      "  [ 23 134]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "\n",
    "numFolds = 10\n",
    "EPOCHS = 15\n",
    "kf = KFold(n_splits = numFolds, shuffle = True, random_state = 1234)\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "train_split = []\n",
    "test_split = []\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(new_df):\n",
    "    i = i + 1\n",
    "    print('\\nFold', i)\n",
    "    train_split.append(train_index)\n",
    "    test_split.append(test_index)\n",
    "    train_dataset = new_df.iloc[train_index].copy().reset_index()\n",
    "    test_dataset =  new_df.iloc[test_index].copy().reset_index()\n",
    "    train_dataset = data_augment(train_dataset, augdata, aug_prop=[500,200,0], aug_type=0)\n",
    "    train_dataset['list'] = train_dataset['class'].apply(classList)\n",
    "    \n",
    "    print('Training class distribution:')\n",
    "    distrib = train_dataset['class'].value_counts()\n",
    "    print(distrib)\n",
    "    probs = distrib/sum(distrib)\n",
    "    print(probs)\n",
    "\n",
    "    print('Test class distribution:')\n",
    "    distrib = test_dataset['class'].value_counts()\n",
    "    print(distrib)\n",
    "    probs = distrib/sum(distrib)\n",
    "    print(probs)\n",
    "\n",
    "    train_dataset = train_dataset.drop(columns=['class'])\n",
    "    test_dataset = test_dataset.drop(columns=['class'])\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "    testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)    \n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    testing_loader = DataLoader(testing_set, **test_params)\n",
    "   \n",
    "    model = BERTClass()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(model, optimizer, epoch, training_loader)\n",
    "        \n",
    "    dlist, outputs, targets = validation(model, testing_loader)\n",
    "\n",
    "    pred = pd.DataFrame(dlist, columns=['rowid'])\n",
    "    temp = pd.DataFrame(targets)\n",
    "    pred['truth'] = temp.idxmax(axis=1)\n",
    "    temp = pd.DataFrame(outputs)\n",
    "    pred['predictions'] = temp.idxmax(axis=1)\n",
    "    if i == 1:\n",
    "        result = pred\n",
    "    else:\n",
    "        result = pd.concat([result, pred], ignore_index=True)\n",
    "    \n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    cm = multilabel_confusion_matrix(targets, outputs)\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "    print(f\"Confusion matrix:\")\n",
    "    print(cm)   \n",
    "\n",
    "    with open(resultdir + 'model' + str(i) + '_cv','wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "\n",
    "result = result.sort_values(by=['rowid'])\n",
    "result = result.set_index('rowid')\n",
    "result = pd.concat((result, new_df), axis=1, ignore_index=True)\n",
    "result.iloc[result.iloc[:,0]==1,0] = -1\n",
    "result.iloc[result.iloc[:,0]==0,0] = 1\n",
    "result.iloc[result.iloc[:,0]==2,0] = 0\n",
    "result.iloc[result.iloc[:,1]==1,1] = -1\n",
    "result.iloc[result.iloc[:,1]==0,1] = 1\n",
    "result.iloc[result.iloc[:,1]==2,1] = 0\n",
    "result = result.rename(columns={0: 'class', 1: 'predicted', 2: 'tweet'})\n",
    "result = result.drop(columns=[3,4,5])\n",
    "result.to_csv(resultdir + 'bert_results.csv', index=False)\n",
    "\n",
    "with open(resultdir + 'train_split_cv','wb') as split_file:\n",
    "    pickle.dump(train_split, split_file)        \n",
    "\n",
    "with open(resultdir + 'test_split_cv','wb') as split_file:\n",
    "    pickle.dump(test_split, split_file)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84608dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:0.8263114415530772\n",
      "[[1049  135  218]\n",
      " [  81 1349  135]\n",
      " [ 141  131 1603]]\n",
      "Class Anti:\n",
      "   Precision = 0.8253343823760818\n",
      "   Recall = 0.7482168330955777\n",
      "   F-measure = 0.784885895997007\n",
      "Class Neutral:\n",
      "   Precision = 0.8352941176470589\n",
      "   Recall = 0.8619808306709266\n",
      "   F-measure = 0.8484276729559749\n",
      "Class Pro:\n",
      "   Precision = 0.8195296523517382\n",
      "   Recall = 0.8549333333333333\n",
      "   F-measure = 0.8368572174367007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('\\nAccuracy:' + str(accuracy_score(result['class'], result['predicted'])))\n",
    "cm = confusion_matrix(result['class'], result['predicted'])\n",
    "\n",
    "print(cm)\n",
    "print('Class Anti:')\n",
    "prec = cm[0][0]/cm[:,0].sum()\n",
    "recall = cm[0][0]/cm[0,:].sum()\n",
    "f1 = 2*prec*recall/(prec + recall)\n",
    "print('   Precision =', prec)\n",
    "print('   Recall =', recall)\n",
    "print('   F-measure =', f1)\n",
    "print('Class Neutral:')\n",
    "prec = cm[1][1]/cm[:,1].sum()\n",
    "recall = cm[1][1]/cm[1,:].sum()\n",
    "f1 = 2*prec*recall/(prec + recall)\n",
    "print('   Precision =', prec)\n",
    "print('   Recall =', recall)\n",
    "print('   F-measure =', f1)\n",
    "print('Class Pro:')\n",
    "prec = cm[2][2]/cm[:,2].sum()\n",
    "recall = cm[2][2]/cm[2,:].sum()\n",
    "f1 = 2*prec*recall/(prec + recall)\n",
    "print('   Precision =', prec)\n",
    "print('   Recall =', recall)\n",
    "print('   F-measure =', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930cf06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
