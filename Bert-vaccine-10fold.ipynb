{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256c0ffa",
   "metadata": {},
   "source": [
    "# Results for Feb 3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3177c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "datadir = '../data/'\n",
    "resultdir = '../results_vaccine/'\n",
    "numFolds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d56973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceca3973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>list</th>\n",
       "      <th>rowid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@DickDugan @maddow They make much more selling...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@theheraldsun How does an unvaccinated person ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@cameronjowens @leighsales We have much higher...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Officials I trust fear there is not enough int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My daughter 30 years old, care worker. Double ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10831</th>\n",
       "      <td>M?ori tribe tells anti-Covid vaccine protester...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>10831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10832</th>\n",
       "      <td>Anti-vaccine protesters display Nazi symbols o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>10832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10833</th>\n",
       "      <td>@Joc_face @TravisR96776163 @ElijahSchaffer If ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>10833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10834</th>\n",
       "      <td>@ponderousthings @1NewsNZ @jordyn_rudd \"The SA...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>10834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10835</th>\n",
       "      <td>@JohnAndersonAO My concern is that it's a COVI...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>10835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10836 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  class    list  rowid\n",
       "0      @DickDugan @maddow They make much more selling...      1  [1, 0]      0\n",
       "1      @theheraldsun How does an unvaccinated person ...      1  [1, 0]      1\n",
       "2      @cameronjowens @leighsales We have much higher...      1  [1, 0]      2\n",
       "3      Officials I trust fear there is not enough int...      1  [1, 0]      3\n",
       "4      My daughter 30 years old, care worker. Double ...      1  [1, 0]      4\n",
       "...                                                  ...    ...     ...    ...\n",
       "10831  M?ori tribe tells anti-Covid vaccine protester...      1  [1, 0]  10831\n",
       "10832  Anti-vaccine protesters display Nazi symbols o...      1  [1, 0]  10832\n",
       "10833  @Joc_face @TravisR96776163 @ElijahSchaffer If ...      1  [1, 0]  10833\n",
       "10834  @ponderousthings @1NewsNZ @jordyn_rudd \"The SA...      1  [1, 0]  10834\n",
       "10835  @JohnAndersonAO My concern is that it's a COVI...      1  [1, 0]  10835\n",
       "\n",
       "[10836 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(datadir + \"vaccine.csv\")\n",
    "df = df.drop(columns=['class','vaccine','pro','anti','neutral'])\n",
    "df['related'] = df['vax_class']\n",
    "df['unrelated'] = 1 - df['vax_class']\n",
    "df = df.rename(columns={'vax_class':'class'})\n",
    "df['list'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['tweet', 'class', 'list']].copy()\n",
    "new_df['rowid'] = new_df.index\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cfaf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 150\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33a7d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.tweet\n",
    "        self.targets = self.data.list\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'rowid': self.data.loc[index, 'rowid'],\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb2c128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db82c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82fd3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch, data_loader):\n",
    "    model.train()\n",
    "    for _,data in enumerate(data_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "998f46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, data_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    dlist = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            dlist.extend(data['rowid'].cpu().detach().numpy().tolist())\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return dlist, fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e07618",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Training class distribution:\n",
      "0    5392\n",
      "1    4360\n",
      "Name: class, dtype: int64\n",
      "0    0.552912\n",
      "1    0.447088\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      "0    574\n",
      "1    510\n",
      "Name: class, dtype: int64\n",
      "0    0.52952\n",
      "1    0.47048\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (10836, 4)\n",
      "TRAIN Dataset: (9752, 4)\n",
      "TEST Dataset: (1084, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7539483904838562\n",
      "Epoch: 1, Loss:  0.013189608231186867\n",
      "Epoch: 2, Loss:  0.008419126272201538\n",
      "Epoch: 3, Loss:  0.006067648064345121\n",
      "Epoch: 4, Loss:  0.06265126913785934\n",
      "Epoch: 5, Loss:  0.007400728762149811\n",
      "Epoch: 6, Loss:  0.004239530302584171\n",
      "Epoch: 7, Loss:  0.0012353351339697838\n",
      "Epoch: 8, Loss:  0.0009555773576721549\n",
      "Epoch: 9, Loss:  0.00047716376138851047\n",
      "Accuracy Score = 0.9898523985239852\n",
      "F1 Score (Micro) = 0.9898523985239852\n",
      "F1 Score (Macro) = 0.9898134835511339\n",
      "Confusion matrix:\n",
      "[[[570   4]\n",
      "  [  7 503]]\n",
      "\n",
      " [[503   7]\n",
      "  [  4 570]]]\n",
      "\n",
      "Fold 2\n",
      "Training class distribution:\n",
      "0    5369\n",
      "1    4383\n",
      "Name: class, dtype: int64\n",
      "0    0.550554\n",
      "1    0.449446\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      "0    597\n",
      "1    487\n",
      "Name: class, dtype: int64\n",
      "0    0.550738\n",
      "1    0.449262\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (10836, 4)\n",
      "TRAIN Dataset: (9752, 4)\n",
      "TEST Dataset: (1084, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6488358378410339\n",
      "Epoch: 1, Loss:  0.25727513432502747\n",
      "Epoch: 2, Loss:  0.33283987641334534\n",
      "Epoch: 3, Loss:  0.006529924925416708\n",
      "Epoch: 4, Loss:  0.013260958716273308\n",
      "Epoch: 5, Loss:  0.0006545749492943287\n",
      "Epoch: 6, Loss:  0.0005049912724643946\n",
      "Epoch: 7, Loss:  0.0006394694792106748\n",
      "Epoch: 8, Loss:  0.00029857089975848794\n",
      "Epoch: 9, Loss:  0.00029682149761356413\n",
      "Accuracy Score = 0.988929889298893\n",
      "F1 Score (Micro) = 0.988929889298893\n",
      "F1 Score (Macro) = 0.988814710100812\n",
      "Confusion matrix:\n",
      "[[[591   6]\n",
      "  [  6 481]]\n",
      "\n",
      " [[481   6]\n",
      "  [  6 591]]]\n",
      "\n",
      "Fold 3\n",
      "Training class distribution:\n",
      "0    5353\n",
      "1    4399\n",
      "Name: class, dtype: int64\n",
      "0    0.548913\n",
      "1    0.451087\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      "0    613\n",
      "1    471\n",
      "Name: class, dtype: int64\n",
      "0    0.565498\n",
      "1    0.434502\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (10836, 4)\n",
      "TRAIN Dataset: (9752, 4)\n",
      "TEST Dataset: (1084, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7504503726959229\n",
      "Epoch: 1, Loss:  0.13572923839092255\n",
      "Epoch: 2, Loss:  0.11758451908826828\n",
      "Epoch: 3, Loss:  0.005898375064134598\n",
      "Epoch: 4, Loss:  0.004119442775845528\n",
      "Epoch: 5, Loss:  0.0009359029936604202\n",
      "Epoch: 6, Loss:  0.000823172798845917\n",
      "Epoch: 7, Loss:  0.000591269985307008\n",
      "Epoch: 8, Loss:  0.0003145608934573829\n",
      "Epoch: 9, Loss:  0.0004518660716712475\n",
      "Accuracy Score = 0.9935424354243543\n",
      "F1 Score (Micro) = 0.9935424354243543\n",
      "F1 Score (Macro) = 0.9934344810877926\n",
      "Confusion matrix:\n",
      "[[[608   5]\n",
      "  [  2 469]]\n",
      "\n",
      " [[469   2]\n",
      "  [  5 608]]]\n",
      "\n",
      "Fold 4\n",
      "Training class distribution:\n",
      "0    5367\n",
      "1    4385\n",
      "Name: class, dtype: int64\n",
      "0    0.550349\n",
      "1    0.449651\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      "0    599\n",
      "1    485\n",
      "Name: class, dtype: int64\n",
      "0    0.552583\n",
      "1    0.447417\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (10836, 4)\n",
      "TRAIN Dataset: (9752, 4)\n",
      "TEST Dataset: (1084, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7121485471725464\n",
      "Epoch: 1, Loss:  0.0629415437579155\n",
      "Epoch: 2, Loss:  0.009030823595821857\n",
      "Epoch: 3, Loss:  0.015091138891875744\n",
      "Epoch: 4, Loss:  0.0029725474305450916\n",
      "Epoch: 5, Loss:  0.0029793090652674437\n",
      "Epoch: 6, Loss:  0.7497513294219971\n",
      "Epoch: 7, Loss:  0.0004933716263622046\n",
      "Epoch: 8, Loss:  0.0013040483463555574\n",
      "Epoch: 9, Loss:  0.0007441674824804068\n",
      "Accuracy Score = 0.9907749077490775\n",
      "F1 Score (Micro) = 0.9907749077490775\n",
      "F1 Score (Macro) = 0.9906824183078446\n",
      "Confusion matrix:\n",
      "[[[591   8]\n",
      "  [  2 483]]\n",
      "\n",
      " [[483   2]\n",
      "  [  8 591]]]\n",
      "\n",
      "Fold 5\n",
      "Training class distribution:\n",
      "0    5384\n",
      "1    4368\n",
      "Name: class, dtype: int64\n",
      "0    0.552092\n",
      "1    0.447908\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      "0    582\n",
      "1    502\n",
      "Name: class, dtype: int64\n",
      "0    0.5369\n",
      "1    0.4631\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (10836, 4)\n",
      "TRAIN Dataset: (9752, 4)\n",
      "TEST Dataset: (1084, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7428121566772461\n",
      "Epoch: 1, Loss:  0.02162463776767254\n",
      "Epoch: 2, Loss:  0.0059189279563724995\n",
      "Epoch: 3, Loss:  0.003084505908191204\n",
      "Epoch: 4, Loss:  0.001992512959986925\n",
      "Epoch: 5, Loss:  0.833691418170929\n",
      "Epoch: 6, Loss:  0.0018920109141618013\n",
      "Epoch: 7, Loss:  0.0009755956125445664\n",
      "Epoch: 8, Loss:  0.0015147097874432802\n",
      "Epoch: 9, Loss:  0.0003533825511112809\n",
      "Accuracy Score = 0.9916974169741697\n",
      "F1 Score (Micro) = 0.9921622867680959\n",
      "F1 Score (Macro) = 0.992153275310774\n",
      "Confusion matrix:\n",
      "[[[578   4]\n",
      "  [  4 498]]\n",
      "\n",
      " [[497   5]\n",
      "  [  4 578]]]\n",
      "\n",
      "Fold 6\n",
      "Training class distribution:\n",
      "0    5333\n",
      "1    4419\n",
      "Name: class, dtype: int64\n",
      "0    0.546862\n",
      "1    0.453138\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      "0    633\n",
      "1    451\n",
      "Name: class, dtype: int64\n",
      "0    0.583948\n",
      "1    0.416052\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (10836, 4)\n",
      "TRAIN Dataset: (9752, 4)\n",
      "TEST Dataset: (1084, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6218694448471069\n",
      "Epoch: 1, Loss:  0.03179900348186493\n",
      "Epoch: 2, Loss:  0.2825593948364258\n",
      "Epoch: 3, Loss:  0.005758652463555336\n",
      "Epoch: 4, Loss:  0.0025708521716296673\n",
      "Epoch: 5, Loss:  0.0019084358355030417\n",
      "Epoch: 6, Loss:  0.0008333807345479727\n",
      "Epoch: 7, Loss:  0.0024322629906237125\n",
      "Epoch: 8, Loss:  0.0005052213091403246\n",
      "Epoch: 9, Loss:  0.0004777576250489801\n",
      "Accuracy Score = 0.9944649446494465\n",
      "F1 Score (Micro) = 0.9944649446494465\n",
      "F1 Score (Macro) = 0.9943115626410302\n",
      "Confusion matrix:\n",
      "[[[628   5]\n",
      "  [  1 450]]\n",
      "\n",
      " [[450   1]\n",
      "  [  5 628]]]\n",
      "\n",
      "Fold 7\n",
      "Training class distribution:\n",
      "0    5375\n",
      "1    4378\n",
      "Name: class, dtype: int64\n",
      "0    0.551112\n",
      "1    0.448888\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      "0    591\n",
      "1    492\n",
      "Name: class, dtype: int64\n",
      "0    0.545706\n",
      "1    0.454294\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (10836, 4)\n",
      "TRAIN Dataset: (9753, 4)\n",
      "TEST Dataset: (1083, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7362067103385925\n",
      "Epoch: 1, Loss:  0.028943557292222977\n",
      "Epoch: 2, Loss:  0.12668374180793762\n",
      "Epoch: 3, Loss:  0.0036054830998182297\n",
      "Epoch: 4, Loss:  0.0017478989902883768\n",
      "Epoch: 5, Loss:  0.023455841466784477\n",
      "Epoch: 6, Loss:  0.002038294682279229\n",
      "Epoch: 7, Loss:  0.32143735885620117\n",
      "Epoch: 8, Loss:  0.0003479243896435946\n",
      "Epoch: 9, Loss:  0.00017948159074876457\n",
      "Accuracy Score = 0.9879963065558633\n",
      "F1 Score (Micro) = 0.9879963065558633\n",
      "F1 Score (Macro) = 0.9878757346767423\n",
      "Confusion matrix:\n",
      "[[[589   2]\n",
      "  [ 11 481]]\n",
      "\n",
      " [[481  11]\n",
      "  [  2 589]]]\n",
      "\n",
      "Fold 8\n",
      "Training class distribution:\n",
      "0    5370\n",
      "1    4383\n",
      "Name: class, dtype: int64\n",
      "0    0.5506\n",
      "1    0.4494\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      "0    596\n",
      "1    487\n",
      "Name: class, dtype: int64\n",
      "0    0.550323\n",
      "1    0.449677\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (10836, 4)\n",
      "TRAIN Dataset: (9753, 4)\n",
      "TEST Dataset: (1083, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6764918565750122\n",
      "Epoch: 1, Loss:  0.024465691298246384\n",
      "Epoch: 2, Loss:  0.009615588933229446\n",
      "Epoch: 3, Loss:  0.06839737296104431\n",
      "Epoch: 4, Loss:  0.0027164011262357235\n",
      "Epoch: 5, Loss:  0.000872431555762887\n",
      "Epoch: 6, Loss:  0.0009081641328521073\n",
      "Epoch: 7, Loss:  0.0005737596075050533\n",
      "Epoch: 8, Loss:  0.002161347074434161\n",
      "Epoch: 9, Loss:  0.00040542270289734006\n",
      "Accuracy Score = 0.9944598337950139\n",
      "F1 Score (Micro) = 0.9944598337950139\n",
      "F1 Score (Macro) = 0.9944031393409865\n",
      "Confusion matrix:\n",
      "[[[593   3]\n",
      "  [  3 484]]\n",
      "\n",
      " [[484   3]\n",
      "  [  3 593]]]\n",
      "\n",
      "Fold 9\n",
      "Training class distribution:\n",
      "0    5371\n",
      "1    4382\n",
      "Name: class, dtype: int64\n",
      "0    0.550702\n",
      "1    0.449298\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      "0    595\n",
      "1    488\n",
      "Name: class, dtype: int64\n",
      "0    0.5494\n",
      "1    0.4506\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (10836, 4)\n",
      "TRAIN Dataset: (9753, 4)\n",
      "TEST Dataset: (1083, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7305500507354736\n",
      "Epoch: 1, Loss:  0.020016146823763847\n",
      "Epoch: 2, Loss:  0.011361181735992432\n",
      "Epoch: 3, Loss:  0.015954483300447464\n",
      "Epoch: 4, Loss:  0.0035238948184996843\n",
      "Epoch: 5, Loss:  0.0017681794706732035\n",
      "Epoch: 6, Loss:  0.003286840161308646\n",
      "Epoch: 7, Loss:  0.001761268824338913\n",
      "Epoch: 8, Loss:  0.000645893276669085\n",
      "Epoch: 9, Loss:  0.0009307905565947294\n",
      "Accuracy Score = 0.9824561403508771\n",
      "F1 Score (Micro) = 0.9829257037378866\n",
      "F1 Score (Macro) = 0.9827909499457617\n",
      "Confusion matrix:\n",
      "[[[588   7]\n",
      "  [ 11 477]]\n",
      "\n",
      " [[476  12]\n",
      "  [  7 588]]]\n",
      "\n",
      "Fold 10\n",
      "Training class distribution:\n",
      "0    5380\n",
      "1    4373\n",
      "Name: class, dtype: int64\n",
      "0    0.551625\n",
      "1    0.448375\n",
      "Name: class, dtype: float64\n",
      "Test class distribution:\n",
      "0    586\n",
      "1    497\n",
      "Name: class, dtype: int64\n",
      "0    0.54109\n",
      "1    0.45891\n",
      "Name: class, dtype: float64\n",
      "FULL Dataset: (10836, 4)\n",
      "TRAIN Dataset: (9753, 4)\n",
      "TEST Dataset: (1083, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7787419557571411\n",
      "Epoch: 1, Loss:  0.012834059074521065\n",
      "Epoch: 2, Loss:  0.007244109641760588\n",
      "Epoch: 3, Loss:  0.5341516733169556\n",
      "Epoch: 4, Loss:  0.007459256798028946\n",
      "Epoch: 5, Loss:  0.002217660192400217\n",
      "Epoch: 6, Loss:  0.0012661612126976252\n",
      "Epoch: 7, Loss:  0.0008571565849706531\n",
      "Epoch: 8, Loss:  0.0007006109226495028\n",
      "Epoch: 9, Loss:  0.0002969239139929414\n",
      "Accuracy Score = 0.989843028624192\n",
      "F1 Score (Micro) = 0.989843028624192\n",
      "F1 Score (Macro) = 0.9897691993902571\n",
      "Confusion matrix:\n",
      "[[[582   4]\n",
      "  [  7 490]]\n",
      "\n",
      " [[490   7]\n",
      "  [  4 582]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "\n",
    "EPOCHS = 10\n",
    "kf = KFold(n_splits = numFolds, shuffle = True, random_state = 1234)\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "train_split = []\n",
    "test_split = []\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(new_df):\n",
    "    i = i + 1\n",
    "    print('\\nFold', i)\n",
    "    train_split.append(train_index)\n",
    "    test_split.append(test_index)\n",
    "    train_dataset = new_df.iloc[train_index].copy().reset_index()\n",
    "    test_dataset =  new_df.iloc[test_index].copy().reset_index()\n",
    "    \n",
    "    print('Training class distribution:')\n",
    "    distrib = train_dataset['class'].value_counts()\n",
    "    print(distrib)\n",
    "    probs = distrib/sum(distrib)\n",
    "    print(probs)\n",
    "\n",
    "    print('Test class distribution:')\n",
    "    distrib = test_dataset['class'].value_counts()\n",
    "    print(distrib)\n",
    "    probs = distrib/sum(distrib)\n",
    "    print(probs)\n",
    "\n",
    "    train_dataset = train_dataset.drop(columns=['class'])\n",
    "    test_dataset = test_dataset.drop(columns=['class'])\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "    testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)    \n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    testing_loader = DataLoader(testing_set, **test_params)\n",
    "   \n",
    "    model = BERTClass()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(model, optimizer, epoch, training_loader)\n",
    "        \n",
    "    dlist, outputs, targets = validation(model, testing_loader)\n",
    "\n",
    "    pred = pd.DataFrame(dlist, columns=['rowid'])\n",
    "    temp = pd.DataFrame(targets)\n",
    "    pred['truth'] = temp.idxmax(axis=1)\n",
    "    temp = pd.DataFrame(outputs)\n",
    "    pred['predictions'] = temp.idxmax(axis=1)\n",
    "    if i == 1:\n",
    "        result = pred\n",
    "    else:\n",
    "        result = pd.concat([result, pred], ignore_index=True)\n",
    "    \n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    cm = multilabel_confusion_matrix(targets, outputs)\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "    print(f\"Confusion matrix:\")\n",
    "    print(cm)   \n",
    "\n",
    "    with open(resultdir + 'model' + str(i) + '_cv_vax','wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "\n",
    "result = result.sort_values(by=['rowid'])\n",
    "result = result.set_index('rowid')\n",
    "result = pd.concat((result, new_df), axis=1, ignore_index=True)\n",
    "result.iloc[result.iloc[:,0]==1,0] = -1\n",
    "result.iloc[result.iloc[:,0]==0,0] = 1\n",
    "result.iloc[result.iloc[:,0]==2,0] = 0\n",
    "result.iloc[result.iloc[:,1]==1,1] = -1\n",
    "result.iloc[result.iloc[:,1]==0,1] = 1\n",
    "result.iloc[result.iloc[:,1]==2,1] = 0\n",
    "result = result.rename(columns={0: 'class', 1: 'predicted', 2: 'tweet'})\n",
    "result = result.drop(columns=[3,4,5])\n",
    "result.to_csv(resultdir + 'bert_results_vax.csv', index=False)\n",
    "\n",
    "with open(resultdir + 'train_split_cv_vax','wb') as split_file:\n",
    "    pickle.dump(train_split, split_file)        \n",
    "\n",
    "with open(resultdir + 'test_split_cv_vax','wb') as split_file:\n",
    "    pickle.dump(test_split, split_file)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84608dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:0.9905869324473976\n",
      "[[5918   48]\n",
      " [  54 4816]]\n",
      "Class Anti:\n",
      "   Precision = 0.9909578030810449\n",
      "   Recall = 0.9919544083137781\n",
      "   F-measure = 0.9914558552521361\n",
      "Class Neutral:\n",
      "   Precision = 0.9901315789473685\n",
      "   Recall = 0.988911704312115\n",
      "   F-measure = 0.9895212656667352\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('\\nAccuracy:' + str(accuracy_score(result['class'], result['predicted'])))\n",
    "cm = confusion_matrix(result['class'], result['predicted'])\n",
    "\n",
    "print(cm)\n",
    "print('Class Anti:')\n",
    "prec = cm[0][0]/cm[:,0].sum()\n",
    "recall = cm[0][0]/cm[0,:].sum()\n",
    "f1 = 2*prec*recall/(prec + recall)\n",
    "print('   Precision =', prec)\n",
    "print('   Recall =', recall)\n",
    "print('   F-measure =', f1)\n",
    "print('Class Neutral:')\n",
    "prec = cm[1][1]/cm[:,1].sum()\n",
    "recall = cm[1][1]/cm[1,:].sum()\n",
    "f1 = 2*prec*recall/(prec + recall)\n",
    "print('   Precision =', prec)\n",
    "print('   Recall =', recall)\n",
    "print('   F-measure =', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930cf06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
